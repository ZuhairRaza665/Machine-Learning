{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab560
\pard\pardeftab560\slleading20\partightenfactor0

\f0\fs24 \cf0 for q1, the kind of "dataset" within the Datasaurus Dozen using J48 and Random Forest classifiers for Q1, the classification task. I started by looking at baseline models that had default parameters and tracked performance metrics like accuracy, recall, f1-score, and precision. I then adjusted the models by using the Random and Grid search techniques to identify the ideal parameters. Even though the baseline models achieved a moderate level of accuracy, the results were not significantly enhanced by the parameter tuning. This experience brought to light the significance of comprehending algorithm parameters as well as the restrictions on parameter tuning under specific conditions.\
\pard\pardeftab560\slleading20\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf0 I used a Decision Tree Regressor in Q2's regression task to forecast the cost of an automobile. Like Q1, I started with a baseline model that had the default parameters and then used the Random and Grid search methods to look for the best parameter settings. Although the baseline mean squared error (MSE) was high, suggesting subpar performance, the parameter tuning produced only slight gains. This task highlighted the difficulties in making accurate predictions about continuous variables, particularly when using decision tree algorithms. }